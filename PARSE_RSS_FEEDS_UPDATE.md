# âœ… Task parse_rss_feeds - Extract Article Information

## ğŸ¯ Objective

Refactor task Ä‘á»ƒ:
1. âœ… **Fetch RSS feeds** - Request tá»«ng RSS URL
2. âœ… **Parse XML content** - Sá»­ dá»¥ng lxml Ä‘á»ƒ parse RSS
3. âœ… **Extract article info** - Láº¥y thÃ´ng tin chi tiáº¿t cá»§a tá»«ng bÃ i bÃ¡o

## ğŸ”„ Changes

### âŒ Before: `extract_rss_urls` - Chá»‰ extract URLs

```python
@task
def extract_rss_urls(metadata: dict) -> dict:
    # Load RSS URLs
    df = load_output(...)
    
    # Extract URLs only
    df_urls = df[["category", "rss_url"]].copy()
    
    # Save URLs
    save_output(data=df_urls)
    
    return {"url_count": len(df_urls)}
```

**Váº¥n Ä‘á»:**
- âŒ Chá»‰ extract URLs, khÃ´ng fetch content
- âŒ KhÃ´ng parse RSS XML
- âŒ KhÃ´ng láº¥y thÃ´ng tin bÃ i bÃ¡o

### âœ… After: `parse_rss_feeds` - Parse & Extract Articles

```python
@task
def parse_rss_feeds(metadata: dict) -> dict:
    from parsers.rss_feed_parser import RSSFeedParser
    
    # Load RSS URLs
    df = load_output(...)
    
    # Prepare feeds list
    feeds = [
        {"url": row["rss_url"], "category": row["category"]}
        for idx, row in df.iterrows()
    ]
    
    # Parse RSS feeds
    parser = RSSFeedParser(timeout=30, max_retries=3)
    articles = parser.parse_multiple_feeds(feeds)
    
    # Convert to DataFrame
    df_articles = pd.DataFrame(articles)
    
    # Save articles
    save_output(data=df_articles)
    
    return {"article_count": len(df_articles)}
```

**Æ¯u Ä‘iá»ƒm:**
- âœ… Fetch RSS content tá»« URLs
- âœ… Parse XML vá»›i lxml
- âœ… Extract article information
- âœ… Handle errors gracefully

## ğŸ“Š Article Information Extracted

Má»—i article chá»©a:

| Field | Description | Example |
|-------|-------------|---------|
| `title` | TiÃªu Ä‘á» bÃ i bÃ¡o | "Viá»‡t Nam tháº¯ng 3-0..." |
| `link` | URL bÃ i bÃ¡o | "https://vnexpress.net/..." |
| `description` | MÃ´ táº£ ngáº¯n | "Äá»™i tuyá»ƒn Viá»‡t Nam..." |
| `published_date` | NgÃ y publish | "Mon, 16 Feb 2026..." |
| `author` | TÃ¡c giáº£ | "Nguyá»…n VÄƒn A" |
| `category` | Chá»§ Ä‘á» | "Thá»ƒ thao" |
| `guid` | Unique ID | "https://vnexpress.net/..." |
| `thumbnail` | áº¢nh thumbnail | "https://i1-thethao.vnecdn.net/..." |

## ğŸ”§ RSS Feed Parser

### Features:

**1. Fetch RSS Content**
```python
# HTTP request vá»›i retry logic
response = session.get(url, timeout=30)
xml_content = response.text
```

**2. Parse XML vá»›i lxml**
```python
# Parse XML
root = etree.fromstring(xml_content.encode('utf-8'))

# Find all items
items = root.xpath('//item')
```

**3. Extract Article Info**
```python
# Extract tá»«ng field
title = item.find('title').text
link = item.find('link').text
description = item.find('description').text
pubDate = item.find('pubDate').text
```

**4. Handle Multiple Feeds**
```python
# Parse multiple feeds
articles = parser.parse_multiple_feeds([
    {"url": "https://example.com/rss", "category": "News"},
    {"url": "https://example.com/tech.rss", "category": "Tech"}
])
```

## ğŸ“ Example Flow

### Input (RSS URLs):
```
DataFrame (3 feeds):
     category                                    rss_url
0        News  https://vnexpress.net/rss/tin-moi-nhat.rss
1    Business  https://vnexpress.net/rss/kinh-doanh.rss
2       Tech  https://vnexpress.net/rss/so-hoa.rss
```

### Processing:
```
[1/3] Processing: News - https://vnexpress.net/rss/tin-moi-nhat.rss
   ğŸ“¡ Fetching RSS feed...
   âœ… Fetched successfully: 45,230 bytes
   ğŸ“° Found 20 articles
   âœ… Parsed 20 articles successfully
   âœ… Added 20 articles

[2/3] Processing: Business - https://vnexpress.net/rss/kinh-doanh.rss
   ğŸ“¡ Fetching RSS feed...
   âœ… Fetched successfully: 38,120 bytes
   ğŸ“° Found 15 articles
   âœ… Parsed 15 articles successfully
   âœ… Added 15 articles

[3/3] Processing: Tech - https://vnexpress.net/rss/so-hoa.rss
   ğŸ“¡ Fetching RSS feed...
   âœ… Fetched successfully: 42,890 bytes
   ğŸ“° Found 18 articles
   âœ… Parsed 18 articles successfully
   âœ… Added 18 articles

âœ… Total articles parsed: 53
```

### Output (Articles):
```
DataFrame (53 articles):
                                    title  ... thumbnail
0   Viá»‡t Nam tháº¯ng 3-0 trÆ°á»›c ThÃ¡i Lan  ... https://...
1   GiÃ¡ vÃ ng tÄƒng máº¡nh trong tuáº§n qua  ... https://...
2   iPhone 16 ra máº¯t vá»›i nhiá»u tÃ­nh nÄƒng má»›i  ... https://...
...

Columns: ['title', 'link', 'description', 'published_date', 
          'author', 'category', 'guid', 'thumbnail']
```

### Saved as Parquet:
```
File: parse_rss_feeds_2024-02-16.parquet
Size: ~15 KB
Format: Parquet (PyArrow Table)
Records: 53 articles
Schema: {
    title: string,
    link: string,
    description: string,
    published_date: string,
    author: string,
    category: string,
    guid: string,
    thumbnail: string
}
```

## ğŸ›¡ï¸ Error Handling

### Level 1: Individual Feed Error
```python
# Náº¿u 1 feed fail â†’ skip vÃ  continue
try:
    articles = parser.parse_rss_url(url, category)
except Exception as e:
    logger.error(f"Failed to parse feed: {e}")
    continue  # Skip to next feed
```

### Level 2: No Articles Found
```python
# Náº¿u khÃ´ng cÃ³ articles â†’ return empty result
if len(articles) == 0:
    return {
        "article_count": 0,
        "success": False
    }
```

### Level 3: Parser Error
```python
# Náº¿u parser fail â†’ return error result
except Exception as e:
    logger.error(f"Failed to parse RSS feeds: {e}")
    return {
        "article_count": 0,
        "success": False,
        "error": str(e)
    }
```

## âš¡ Performance

### Retry Logic:
```python
# Automatic retry on failure
retry_strategy = Retry(
    total=3,
    backoff_factor=1,
    status_forcelist=[429, 500, 502, 503, 504]
)
```

### Timeout:
```python
# 30 seconds timeout per request
response = session.get(url, timeout=30)
```

### Parallel Processing (Future):
```python
# TODO: Add parallel processing for multiple feeds
with ThreadPoolExecutor(max_workers=5) as executor:
    futures = [executor.submit(parser.parse_rss_url, feed) for feed in feeds]
```

## ğŸ“Š Metadata

Updated metadata includes article count:

```python
{
    "task_id": "parse_rss_feeds",
    "run_id": "2024-02-16",
    "article_count": 53,
    "feed_count": 3,
    "storage_path": "/tmp/.../parse_rss_feeds_2024-02-16.parquet",
    "success": True
}
```

## ğŸš€ Next Stage Integration

Downstream tasks cÃ³ thá»ƒ process articles:

```python
@task
def process_articles(metadata: dict):
    # Load articles
    df = load_output(task_id="parse_rss_feeds", ...)
    
    # Process articles
    for idx, row in df.iterrows():
        title = row['title']
        link = row['link']
        category = row['category']
        
        # Extract full content, vectorize, etc.
        ...
```

## ğŸ“ Files Created

1. **`plugins/parsers/rss_feed_parser.py`** - RSS parser vá»›i lxml
2. **`dags/news-dag.py`** - Updated task

## âœ… Benefits Summary

1. **Complete Article Info**
   - âœ… Title, link, description
   - âœ… Published date, author
   - âœ… Category, thumbnail

2. **Robust Parsing**
   - âœ… lxml for XML parsing
   - âœ… Retry logic
   - âœ… Error handling

3. **Scalable**
   - âœ… Handle multiple feeds
   - âœ… Efficient storage (parquet)
   - âœ… Ready for parallel processing

4. **Production Ready**
   - âœ… Graceful error handling
   - âœ… Detailed logging
   - âœ… Type-safe with PyArrow

---

**Status**: âœ… UPDATED
**Date**: 2026-02-16
**Task**: `extract_rss_urls` â†’ `parse_rss_feeds`
**Result**: Full article extraction with lxml! ğŸ‰
