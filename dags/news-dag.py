"""
VNExpress ETL Pipeline - TaskFlow API v·ªõi Custom Storage
Kh√¥ng d√πng XCom, thay v√†o ƒë√≥ d√πng TaskStorageManager
"""

from airflow.decorators import dag, task
from airflow.utils.dates import days_ago
from datetime import timedelta
import logging

# Import custom modules
# Import t·ª´ plugins folder (Airflow t·ª± ƒë·ªông add plugins v√†o PYTHONPATH)
from scrapers.rss_scaper import RSSListScraper
from storage.short_memory_manager import (
    init_storage,
    StorageBackend,
    save_output,
    load_output,
    cleanup_output,
)

logger = logging.getLogger(__name__)


# ============================================================================
# DAG CONFIGURATION
# ============================================================================

default_args = {
    "owner": "data_team",
    "depends_on_past": False,
    "email_on_failure": True,
    "email_on_retry": False,
    "retries": 2,
    "retry_delay": timedelta(minutes=5),
    "execution_timeout": timedelta(minutes=30),
}


# ============================================================================
# DAG DEFINITION
# ============================================================================


@dag(
    dag_id="vnexpress_etl_taskflow",
    default_args=default_args,
    description="VNExpress ETL Pipeline v·ªõi TaskFlow API v√† custom storage",
    schedule_interval="0 6 * * *",  # Daily at 6 AM
    start_date=days_ago(1),
    catchup=False,
    max_active_runs=1,
    tags=["vnexpress", "etl", "taskflow", "stage-1"],
)
def vnexpress_etl_pipeline():
    """
    Main ETL Pipeline cho VNExpress
    Stage 1: Scrape RSS list
    """

    @task
    def init_storage_backend(**context):
        """
        Task 0: Initialize storage backend
        C√≥ th·ªÉ switch gi·ªØa FILE v√† REDIS t·∫°i ƒë√¢y
        """
        logger.info("üîß Initializing storage backend...")

        # Option 1: File-based storage (default)
        init_storage(backend=StorageBackend.FILE, base_path="/tmp/airflow_task_storage")

        # Option 2: Redis storage (uncomment khi ready)
        # init_storage(
        #     backend=StorageBackend.REDIS,
        #     host='localhost',
        #     port=6379,
        #     db=0,
        #     ttl=86400  # 24 hours
        # )

        logger.info("‚úÖ Storage backend initialized")
        return True

    @task
    def scrape_rss_list(**context) -> dict:
        """
        Task 1: Scrape RSS list t·ª´ VNExpress

        Returns:
            dict with metadata
        """
        logger.info("=" * 80)
        logger.info("TASK 1: Scraping RSS List")
        logger.info("=" * 80)

        # Get run_id t·ª´ context
        run_id = context["ds"]  # YYYY-MM-DD
        task_id = context["task"].task_id

        # Scrape RSS list
        scraper = RSSListScraper(timeout=30, max_retries=3)
        rss_list = scraper.scrape_rss_list()

        if not rss_list:
            raise Exception("Failed to scrape RSS list")

        logger.info(f"‚úÖ Scraped {len(rss_list)} RSS feeds")

        # Save to storage thay v√¨ XCom
        storage_path = save_output(task_id=task_id, data=rss_list, run_id=run_id)

        logger.info(f"üíæ Saved to storage: {storage_path}")

        # Log sample
        for item in rss_list[:3]:
            logger.info(f"  - {item['category']}: {item['rss_url']}")

        # Return metadata (lightweight) thay v√¨ full data
        return {
            "task_id": task_id,
            "run_id": run_id,
            "count": len(rss_list),
            "storage_path": storage_path,
        }

    @task
    def validate_rss_list(metadata: dict) -> dict:
        """
        Task 2: Validate RSS list

        Args:
            metadata: Metadata t·ª´ task tr∆∞·ªõc

        Returns:
            Validation results
        """
        logger.info("=" * 80)
        logger.info("TASK 2: Validating RSS List")
        logger.info("=" * 80)

        # Load data t·ª´ storage
        rss_list = load_output(task_id=metadata["task_id"], run_id=metadata["run_id"])

        logger.info(f"üìÇ Loaded {len(rss_list)} feeds from storage")

        # Validation checks
        validation_errors = []

        # Check 1: Required fields
        for idx, item in enumerate(rss_list):
            if not item.get("category"):
                validation_errors.append(f"Feed {idx}: Missing category")
            if not item.get("rss_url") or not item["rss_url"].startswith("http"):
                validation_errors.append(f"Feed {idx}: Invalid URL")

        # Check 2: Duplicates
        urls = [item["rss_url"] for item in rss_list]
        if len(urls) != len(set(urls)):
            validation_errors.append("Duplicate RSS URLs found")

        if validation_errors:
            error_msg = "\n".join(validation_errors)
            raise Exception(f"Validation failed:\n{error_msg}")

        logger.info("‚úÖ Validation passed")

        # Return metadata for next task
        return {
            "task_id": metadata["task_id"],
            "run_id": metadata["run_id"],
            "validated": True,
            "feed_count": len(rss_list),
        }

    @task
    def extract_rss_urls(metadata: dict) -> dict:
        """
        Task 3: Extract RSS URLs cho stage ti·∫øp theo

        Args:
            metadata: Metadata t·ª´ validation task

        Returns:
            Metadata with RSS URLs task_id
        """
        logger.info("=" * 80)
        logger.info("TASK 3: Extracting RSS URLs")
        logger.info("=" * 80)

        # Load data t·ª´ storage
        rss_list = load_output(task_id=metadata["task_id"], run_id=metadata["run_id"])

        # Extract URLs
        rss_urls = [
            {"category": item["category"], "url": item["rss_url"]} for item in rss_list
        ]

        logger.info(f"üìã Extracted {len(rss_urls)} RSS URLs")

        # Save URLs list cho stage ti·∫øp theo (parallel parsing)
        current_task_id = "extract_rss_urls"
        run_id = metadata["run_id"]

        storage_path = save_output(
            task_id=current_task_id, data=rss_urls, run_id=run_id
        )

        logger.info(f"üíæ Saved RSS URLs to: {storage_path}")

        return {
            "task_id": current_task_id,
            "run_id": run_id,
            "url_count": len(rss_urls),
            "ready_for_parsing": True,
        }

    @task
    def cleanup_temp_data(metadata: dict) -> dict:
        """
        Task 4: Cleanup temporary data t·ª´ c√°c task tr∆∞·ªõc

        Args:
            metadata: Metadata t·ª´ task tr∆∞·ªõc
        """
        logger.info("=" * 80)
        logger.info("TASK 4: Cleaning up temporary data")
        logger.info("=" * 80)

        run_id = metadata["run_id"]

        # Cleanup task 1 output (scrape_rss_list)
        deleted_1 = cleanup_output("scrape_rss_list", run_id)
        logger.info(f"üóëÔ∏è  Cleaned scrape_rss_list: {deleted_1}")

        # Note: Kh√¥ng x√≥a extract_rss_urls v√¨ stage ti·∫øp theo c·∫ßn

        logger.info("‚úÖ Cleanup completed")

        return {
            "cleaned": True,
            "ready_for_next_stage": True,
        }

    # ========================================================================
    # TASK DEPENDENCIES
    # ========================================================================

    # Chain tasks
    storage_init = init_storage_backend()
    rss_metadata = scrape_rss_list()
    validation_result = validate_rss_list(rss_metadata)
    extraction_result = extract_rss_urls(validation_result)
    cleanup_result = cleanup_temp_data(extraction_result)

    # Set dependencies
    (
        storage_init
        >> rss_metadata
        >> validation_result
        >> extraction_result
        >> cleanup_result
    )


# Instantiate DAG
dag_instance = vnexpress_etl_pipeline()


# ============================================================================
# DOCUMENTATION
# ============================================================================

dag_instance.doc_md = """
# VNExpress ETL Pipeline - Stage 1

## Architecture
S·ª≠ d·ª•ng TaskFlow API v·ªõi custom storage layer (kh√¥ng d√πng XCom).

## Storage Strategy
- **Backend**: File-based (Parquet) - c√≥ th·ªÉ switch sang Redis
- **Location**: `/tmp/airflow_task_storage/{run_id}/{task_id}/`
- **Format**: Auto-detect (Parquet cho list/DataFrame, JSON cho dict, Pickle cho complex objects)
- **Cleanup**: Automatic cleanup sau khi task s·ª≠ d·ª•ng xong

## Task Flow
```
init_storage_backend
    ‚Üì
scrape_rss_list (save to storage)
    ‚Üì
validate_rss_list (load from storage)
    ‚Üì
extract_rss_urls (save URLs for next stage)
    ‚Üì
cleanup_temp_data (cleanup used data)
```

## Data Flow
1. Task tr·∫£ v·ªÅ **metadata** (lightweight) thay v√¨ full data
2. Data ƒë∆∞·ª£c l∆∞u v√†o **TaskStorageManager**
3. Task ti·∫øp theo **load** data t·ª´ storage
4. Sau khi x·ª≠ l√Ω xong ‚Üí **cleanup** ƒë·ªÉ free space

## Switching to Redis
Ch·ªâ c·∫ßn thay ƒë·ªïi trong `init_storage_backend()`:
```python
init_storage(
    backend=StorageBackend.REDIS,
    host='localhost',
    port=6379,
    ttl=86400
)
```

## Benefits
‚úÖ Kh√¥ng b·ªã gi·ªõi h·∫°n XCom size  
‚úÖ Scalable (d·ªÖ migrate Redis)  
‚úÖ Automatic cleanup  
‚úÖ Support nhi·ªÅu data formats  
‚úÖ Easy debugging (c√≥ th·ªÉ inspect files)  

## Next Stage
Output: `extract_rss_urls` task t·∫°o file ch·ª©a RSS URLs  
‚Üí Stage 2 DAG s·∫Ω load file n√†y v√† parse parallel  
"""
